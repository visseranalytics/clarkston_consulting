{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clarkston Consulting Interview Assignment\n",
    "This notebook analyzes the CustomerExtract.csv file and provides the following:\n",
    "<ol>\n",
    "<li>Reads in the CustomerExtract.csv</li>\n",
    "<li>Completes an initial profile of the data and performs data quality checks that you think are needed e.g. Are there any fields in the extract that are populated with data that are not flagged in the spec (Field Utilized in LEGACY System)<ol><li>Quality checks should be visible in the notebook and output for traceability</li></ol></li>\n",
    "<li>Cleanup any data quality issues, such as special characters in the Name 1 field</li>\n",
    "<li>Perform the transformations included in the spec</li>\n",
    "<li>Output a flat file named CustomerLoad.csv</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f7/p92k225d4t57x80m5zl_8jjh0000gn/T/ipykernel_63089/4113098713.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Spec Document Into Dataframe For Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Spec Doc Into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tvisser/Library/Mobile Documents/com~apple~CloudDocs/Projects/clarkston_consulting/env/lib/python3.9/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    }
   ],
   "source": [
    "# Specify Usable Column Names For Later\n",
    "customer_spec_columns = [\n",
    "    'field_label',\n",
    "    'utilized_in_legacy_system',\n",
    "    'utilized_in_new_system',\n",
    "    'table_name',\n",
    "    'field_name',\n",
    "    'field_length',\n",
    "    'field_datatype',\n",
    "    'field_used',\n",
    "    'field_description',\n",
    "    'required_in_legacy_system',\n",
    "    'required_in_new_system',\n",
    "    'fit_or_gap',\n",
    "    'etl_logic'\n",
    "]\n",
    "\n",
    "# Read Excel Doc into DF\n",
    "customer_spec_df = pd.read_excel(\n",
    "    'Dataset/CustomerSpec.xlsx',\n",
    "    header=None, # Replace Header With Spec Columns Dictionary\n",
    "    skipfooter=0,\n",
    "    skiprows=1,\n",
    "    sheet_name='CustomerSpec',\n",
    "    names=customer_spec_columns,\n",
    "    true_values=['Y'],\n",
    "    false_values=['N']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add \"Pythonic\" Label For Each Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a lookup field to map to the extract\n",
    "customer_spec_df['extract_field_name'] = customer_spec_df['field_label'].str.lower().replace({\n",
    "    r'[^A-Z|a-z|0-9]$': '',\n",
    "    r'[^A-Z|a-z|0-9]': '_'\n",
    "    }, regex=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Spec To Just Used Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_spec_required_columns_df = customer_spec_df.loc[customer_spec_df['field_used'] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV into Pandas dataframe\n",
    "customer_extract_df = pd.read_csv(\n",
    "    'Dataset/CustomerExtract.csv',\n",
    "    sep=',',\n",
    "    header=0,\n",
    "    true_values=['Y'],\n",
    "    false_values=['N'],\n",
    "    dtype=object\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Static Required Fields Specified In Selection Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company_code\n",
    "customer_extract_df['BUKRS'] = 'G100'\n",
    "\n",
    "# sales_organization\n",
    "customer_extract_df['VKORG'] = 'G100'\n",
    "\n",
    "# distrubtion_channel\n",
    "customer_extract_df['VTWEG'] = '20'\n",
    "\n",
    "# division\n",
    "customer_extract_df['SPART'] = '10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Potentially Used Fields From Legacy System Not Required In Spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL FIELDS USED IN LEGACY SYSTEM ARE IN USE IN THE NEW SYSTEM\n"
     ]
    }
   ],
   "source": [
    "unused_fields = customer_spec_df.loc[(customer_spec_df['field_used'] == False) & (customer_spec_df['utilized_in_legacy_system'] == True)]\n",
    "\n",
    "if unused_fields.empty:\n",
    "    print('ALL FIELDS USED IN LEGACY SYSTEM ARE IN USE IN THE NEW SYSTEM')\n",
    "else:\n",
    "    for field in unused_fields['field_name'].itertuples():\n",
    "        rows_with_data = len(customer_spec_df.loc[customer_spec_df[field].dropna()])\n",
    "        if rows_with_data > 0:\n",
    "            print(field, 'HAS DATA UNUSED IN NEW SYSTEM.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Unused Columns For Simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING 137 UNUSED COLUMNS\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop = [col for col in customer_extract_df if col not in customer_spec_required_columns_df['field_name'].to_list()]\n",
    "\n",
    "customer_extract_df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "print(\"DROPPING\", len(cols_to_drop), \"UNUSED COLUMNS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Columns To Friendly Names From Spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Extract Columns From Spec\n",
    "customer_extract_column_dict = {}\n",
    "\n",
    "for index, field_name, field_label in customer_spec_required_columns_df[['field_name', 'field_label']].itertuples():\n",
    "    # Remove trailing special characters\n",
    "    field_label = re.sub(r'[^A-Z|a-z|0-9]$', '', field_label)\n",
    "    # Replace remaining special characters with underscores\n",
    "    field_label = re.sub(r'[^A-Z|a-z|0-9]', '_', field_label)\n",
    "    # Lower Case field label\n",
    "    field_label = field_label.lower()\n",
    "    if field_label not in customer_extract_column_dict.values() and not customer_extract_column_dict.get(field_name):\n",
    "        customer_extract_column_dict.update({field_name: field_label})\n",
    "\n",
    "customer_extract_df.rename(columns=customer_extract_column_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast Fields To Proper Types Based On Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f7/p92k225d4t57x80m5zl_8jjh0000gn/T/ipykernel_63089/3255789744.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  customer_extract_df[field_label] = pd.to_datetime(customer_extract_df[field_label], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# Update Data Types Using Spec\n",
    "data_type_map = {\n",
    "    'CHAR': \"str\",\n",
    "    'NUMC': \"numeric\",\n",
    "    'DATS': \"datetime\",\n",
    "    'LANG': \"str\",\n",
    "    'CURR': \"numeric\",\n",
    "    'CUKY': \"numeric\",\n",
    "    'TIMS': \"datetime\",\n",
    "    'QUAN': \"numeric\",\n",
    "    'UNIT': \"numeric\",\n",
    "}\n",
    "\n",
    "for index, field_label, field_datatype in customer_spec_required_columns_df[['extract_field_name', 'field_datatype']].itertuples():\n",
    "    if field_label in customer_extract_df.columns:\n",
    "        data_type = data_type_map[field_datatype]\n",
    "        if data_type == \"datetime\":\n",
    "            customer_extract_df[field_label] = pd.to_datetime(customer_extract_df[field_label], errors='coerce')\n",
    "        elif data_type == \"numeric\":\n",
    "            customer_extract_df[field_label] = pd.to_numeric(customer_extract_df[field_label], errors='coerce')\n",
    "        elif data_type == \"str\":\n",
    "            customer_extract_df[field_label] = customer_extract_df[field_label].astype(str)\n",
    "\n",
    "# Replace Nulls With Proper Nulls\n",
    "customer_extract_df.replace(\"nan\", None, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check For Missing Required Fields In Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL REQUIRED FIELDS IN NEW SYSTEM EXIST IN EXTRACT\n"
     ]
    }
   ],
   "source": [
    "# Find any required columns in the extract missing from the spec\n",
    "required_fields = customer_spec_df['extract_field_name'].loc[customer_spec_df['required_in_new_system'] == True]\n",
    "\n",
    "missing_fields = []\n",
    "for col in required_fields:\n",
    "    if col not in customer_extract_df.columns:\n",
    "        missing_fields.append(col)\n",
    "\n",
    "if missing_fields:\n",
    "    print(\"THE FOLLOWING COLUMNS ARE MISSING FROM THE EXTRACT AND ARE REQUIRED IN THE LEGACY SYSTEM\")\n",
    "    [print(fields) for fields in missing_fields]\n",
    "else:\n",
    "    print('ALL REQUIRED FIELDS IN NEW SYSTEM EXIST IN EXTRACT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check For Truncation In String Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO TRUNCATED COLUMNS FOUND\n"
     ]
    }
   ],
   "source": [
    "# Check For Truncation\n",
    "field_data_types = customer_spec_df[['extract_field_name', 'field_datatype', 'field_length']].loc[customer_spec_df['utilized_in_new_system'] == True]\n",
    "\n",
    "has_truncation = False\n",
    "for index, field_name, field_datatype, field_length in field_data_types.itertuples():\n",
    "    # Check for truncation\n",
    "    if data_type_map[field_datatype] == \"str\" and field_name in customer_extract_df.columns:\n",
    "        truncated_records = customer_extract_df[field_name].loc[\n",
    "            (customer_extract_df[field_name].astype(str).str.len() > field_length) & (customer_extract_df[field_name].isna() == False)\n",
    "        ].drop_duplicates()\n",
    "\n",
    "        if not truncated_records.empty:\n",
    "            print(field_name, 'has', len(truncated_records), 'truncated records. Max length for column is', field_length)\n",
    "            has_trunction = True\n",
    "\n",
    "if not has_truncation:\n",
    "    print('NO TRUNCATED COLUMNS FOUND')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Special Characters From String Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_columns = [col for col in customer_extract_df.select_dtypes(include='object').columns]\n",
    "\n",
    "customer_extract_df[string_columns] = customer_extract_df[string_columns].replace(r'[^A-Z|a-z|0-9|\\,]', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Output With Original Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_columns_dict = {}\n",
    "\n",
    "for original_col, friendly_col in customer_extract_column_dict.items():\n",
    "    original_columns_dict[friendly_col] = original_col\n",
    "\n",
    "customer_extract_df.rename(columns=original_columns_dict).to_csv('CustomerLoad.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Output With Friendly Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_extract_df.to_csv('CustomerLoadFriendlyNames.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Process\n",
    "\n",
    "<ul>\n",
    "<li>Convert the spec into a usable format</li>\n",
    "<li>Rename the extract fields with friendly labels from spec</li>\n",
    "<li>Check for any issues in the extract such as truncation, bad types, missing fields, etc</li>\n",
    "<li>Remove special characters from the string fields.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Found Problems (Fixed By Filtering To Used Fields)\n",
    "\n",
    "<ul>\n",
    "    <li>Account Group and Customer Account have ambiguous field names in the extract. There are two fields in the spec with distinct table/column name combos but the extract doesn't specify which is which.</li>\n",
    "    <li>Duplicate SAP Field Descriptions</li>\n",
    "    <li>Leading and trailing special characters for the field descriptions. This is only an issue due to converting the field names in the extract to friendly terms</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
