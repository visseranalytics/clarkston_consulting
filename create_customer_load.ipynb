{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clarkston Consulting Interview Assignment\n",
    "This notebook analyzes the CustomerExtract.csv file and provides the following:\n",
    "<ol>\n",
    "<li>Reads in the CustomerExtract.csv</li>\n",
    "<li>Completes an initial profile of the data and performs data quality checks that you think are needed e.g. Are there any fields in the extract that are populated with data that are not flagged in the spec (Field Utilized in LEGACY System)<ol><li>Quality checks should be visible in the notebook and output for traceability</li></ol></li>\n",
    "<li>Cleanup any data quality issues, such as special characters in the Name 1 field</li>\n",
    "<li>Perform the transformations included in the spec</li>\n",
    "<li>Output a flat file named CustomerLoad.csv</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Spec Document Into Dataframe For Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tvisser/Library/Mobile Documents/com~apple~CloudDocs/Projects/clarkston_consulting/env/lib/python3.9/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    }
   ],
   "source": [
    "# Specify Usable Column Names For Later\n",
    "customer_spec_columns = [\n",
    "    'field_label',\n",
    "    'utilized_in_legacy_system',\n",
    "    'utilized_in_new_system',\n",
    "    'table_name',\n",
    "    'field_name',\n",
    "    'field_length',\n",
    "    'field_datatype',\n",
    "    'field_used',\n",
    "    'field_description',\n",
    "    'required_in_legacy_system',\n",
    "    'required_in_new_system',\n",
    "    'fit_or_gap',\n",
    "    'etl_logic'\n",
    "]\n",
    "\n",
    "# Read Excel Doc into DF\n",
    "customer_spec_df = pd.read_excel(\n",
    "    'Dataset/CustomerSpec.xlsx',\n",
    "    header=None, # Replace Header With Spec Columns Dictionary\n",
    "    skipfooter=0,\n",
    "    skiprows=1,\n",
    "    sheet_name='CustomerSpec',\n",
    "    names=customer_spec_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace N/Y With Booleans\n",
    "boolean_columns = ['utilized_in_legacy_system', 'utilized_in_new_system', 'field_used']\n",
    "customer_spec_df[boolean_columns] = customer_spec_df[boolean_columns].replace({'Y': True, 'N' :False})\n",
    "\n",
    "# Add a lookup field to map to the extract\n",
    "customer_spec_df['extract_field_name'] = customer_spec_df['field_label'].str.lower().replace({\n",
    "    r'[^A-Z|a-z|0-9]$': '',\n",
    "    r'[^A-Z|a-z|0-9]': '_'\n",
    "    }, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV into Pandas dataframe\n",
    "customer_extract_df = pd.read_csv(\n",
    "    'Dataset/CustomerExtract.csv',\n",
    "    sep=',',\n",
    "    header=0,\n",
    "    true_values=['Y'],\n",
    "    false_values=['N'],\n",
    "    dtype=object\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any unused columns\n",
    "# used_columns = [col for col in customer_spec_df['field_name'].loc[customer_spec_df['field_used'] == True] if col in customer_extract_df.columns]\n",
    "cols_to_drop = [col for col in customer_spec_df['field_name'].loc[customer_spec_df['field_used'] == False] if col in customer_extract_df.columns]\n",
    "\n",
    "customer_extract_df.drop(columns=cols_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Extract Columns From Spec\n",
    "customer_extract_columns = {}\n",
    "\n",
    "for index, field_name, field_label in customer_spec_df[['field_name', 'field_label']].itertuples():\n",
    "    # Remove trailing special characters\n",
    "    field_label = re.sub(r'[^A-Z|a-z|0-9]$', '', field_label)\n",
    "    # Replace remaining special characters with underscores\n",
    "    field_label = re.sub(r'[^A-Z|a-z|0-9]', '_', field_label)\n",
    "    # Lower Case field label\n",
    "    field_label = field_label.lower()\n",
    "    if field_label not in customer_extract_columns.values() and not customer_extract_columns.get(field_name):\n",
    "        customer_extract_columns.update({field_name: field_label})\n",
    "\n",
    "customer_extract_df.rename(columns=customer_extract_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Missing Required Fields Using Selection Logic\n",
    "\n",
    "# company_code\n",
    "customer_extract_df['company_code'] = 'G100'\n",
    "\n",
    "# sales_organization\n",
    "customer_extract_df['sales_organization'] = 'G100'\n",
    "\n",
    "# distrubtion_channel\n",
    "customer_extract_df['distribution_channel'] = '20'\n",
    "\n",
    "# division\n",
    "customer_extract_df['division'] = '10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f7/p92k225d4t57x80m5zl_8jjh0000gn/T/ipykernel_57692/3328011843.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  customer_extract_df[field_label] = pd.to_datetime(customer_extract_df[field_label], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# Update Data Types Using Spec\n",
    "data_type_map = {\n",
    "    'CHAR': \"str\",\n",
    "    'NUMC': \"numeric\",\n",
    "    'DATS': \"datetime\",\n",
    "    'LANG': \"str\",\n",
    "    'CURR': \"numeric\",\n",
    "    'CUKY': \"numeric\",\n",
    "    'TIMS': \"datetime\",\n",
    "    'QUAN': \"numeric\",\n",
    "    'UNIT': \"numeric\",\n",
    "}\n",
    "\n",
    "for index, field_label, field_datatype in customer_spec_df[['extract_field_name', 'field_datatype']].itertuples():\n",
    "    if field_label in customer_extract_df.columns:\n",
    "        data_type = data_type_map[field_datatype]\n",
    "        if data_type == \"datetime\":\n",
    "            customer_extract_df[field_label] = pd.to_datetime(customer_extract_df[field_label], errors='coerce')\n",
    "        elif data_type == \"numeric\":\n",
    "            customer_extract_df[field_label] = pd.to_numeric(customer_extract_df[field_label], errors='coerce')\n",
    "        elif data_type == \"str\":\n",
    "            customer_extract_df[field_label] = customer_extract_df[field_label].astype(str)\n",
    "\n",
    "# Replace Nulls With Proper Nulls\n",
    "customer_extract_df.replace(\"nan\", None, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL REQUIRED FIELDS IN NEW SYSTEM EXIST IN EXTRACT\n"
     ]
    }
   ],
   "source": [
    "# Find any required columns in the extract missing from the spec\n",
    "required_fields = customer_spec_df['extract_field_name'].loc[customer_spec_df['required_in_new_system'] == True]\n",
    "\n",
    "missing_fields = []\n",
    "for col in required_fields:\n",
    "    if col not in customer_extract_df.columns:\n",
    "        missing_fields.append(col)\n",
    "\n",
    "if missing_fields:\n",
    "    print(\"THE FOLLOWING COLUMNS ARE MISSING FROM THE EXTRACT AND ARE REQUIRED IN THE LEGACY SYSTEM\")\n",
    "    [print(fields) for fields in missing_fields]\n",
    "else:\n",
    "    print('ALL REQUIRED FIELDS IN NEW SYSTEM EXIST IN EXTRACT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE FOLLOWING COLUMNS ARE MISSING FROM THE EXTRACT AND ARE REQUIRED IN THE LEGACY SYSTEM\n",
      "account_group\n"
     ]
    }
   ],
   "source": [
    "# Find any required columns in the extract missing from the spec\n",
    "used_fields = customer_spec_df['extract_field_name'].loc[customer_spec_df['field_used'] == True]\n",
    "\n",
    "missing_fields = []\n",
    "for col in used_fields:\n",
    "    if col not in customer_extract_df.columns:\n",
    "        missing_fields.append(col)\n",
    "\n",
    "if missing_fields:\n",
    "    print(\"THE FOLLOWING COLUMNS ARE MISSING FROM THE EXTRACT AND ARE REQUIRED IN THE LEGACY SYSTEM\")\n",
    "    [print(fields) for fields in missing_fields]\n",
    "else:\n",
    "    print('ALL USED FIELDS IN NEW SYSTEM EXIST IN EXTRACT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO TRUNCATED COLUMNS FOUND\n"
     ]
    }
   ],
   "source": [
    "# Check For Truncation\n",
    "field_data_types = customer_spec_df[['extract_field_name', 'field_datatype', 'field_length']].loc[customer_spec_df['utilized_in_new_system'] == True]\n",
    "\n",
    "has_truncation = False\n",
    "for index, field_name, field_datatype, field_length in field_data_types.itertuples():\n",
    "    # Check for truncation\n",
    "    if data_type_map[field_datatype] == \"str\" and field_name in customer_extract_df.columns:\n",
    "        truncated_records = customer_extract_df[field_name].loc[\n",
    "            (customer_extract_df[field_name].astype(str).str.len() > field_length) & (customer_extract_df[field_name].isna() == False)\n",
    "        ].drop_duplicates()\n",
    "\n",
    "        if not truncated_records.empty:\n",
    "            print(field_name, 'has', len(truncated_records), 'truncated records. Max length for column is', field_length)\n",
    "            has_trunction = True\n",
    "\n",
    "if not has_truncation:\n",
    "    print('NO TRUNCATED COLUMNS FOUND')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['customer_account_number',\n",
       " 'country',\n",
       " 'name_1',\n",
       " 'name_2',\n",
       " 'city',\n",
       " 'postal_code',\n",
       " 'region',\n",
       " 'sort_field',\n",
       " 'street',\n",
       " 'telephone_1',\n",
       " 'order_block',\n",
       " 'created_by',\n",
       " 'vendor',\n",
       " 'delivery_block',\n",
       " 'deletion_flag',\n",
       " 'name_3',\n",
       " 'name_4',\n",
       " 'county_code',\n",
       " 'city_code',\n",
       " 'posting_block',\n",
       " 'language',\n",
       " 'tax_number_1',\n",
       " 'tax_number_2',\n",
       " 'transport_zone',\n",
       " 'al_payer_in_doc',\n",
       " 'trading_partner',\n",
       " 'vat_reg__no',\n",
       " 'oi_allowed',\n",
       " 'cust_complexity',\n",
       " 'edi_credit_mgmt',\n",
       " 'eft_credit_mgmt',\n",
       " 'dir_store_dlvry',\n",
       " 'interface',\n",
       " 'labels',\n",
       " 'addenda_info',\n",
       " 'edi_flags',\n",
       " 'attribute_10',\n",
       " 'tax_jur',\n",
       " 'condition_grp_1',\n",
       " 'condition_grp_2',\n",
       " 'condition_grp_3',\n",
       " 'condition_grp_4',\n",
       " 'condition_grp_5',\n",
       " 'tax_number',\n",
       " 'RIC',\n",
       " 'LEGALNAT',\n",
       " 'company_code',\n",
       " 'sales_organization',\n",
       " 'distribution_channel',\n",
       " 'division']"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[col for col in customer_extract_df.select_dtypes(include='object').columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[609], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Find columns with data integrity / formatting needs\u001b[39;00m\n\u001b[1;32m      2\u001b[0m string_columns \u001b[38;5;241m=\u001b[39m [customer_extract_df\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m----> 4\u001b[0m \u001b[43mcustomer_extract_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstring_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^A-Z|a-z|0-9]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Projects/clarkston_consulting/env/lib/python3.9/site-packages/pandas/core/frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Projects/clarkston_consulting/env/lib/python3.9/site-packages/pandas/core/indexes/base.py:6194\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6191\u001b[0m     keyarr \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(keyarr)\n\u001b[1;32m   6193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 6194\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6195\u001b[0m     keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreindex(keyarr)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   6196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Projects/clarkston_consulting/env/lib/python3.9/site-packages/pandas/core/indexes/base.py:6181\u001b[0m, in \u001b[0;36mIndex.get_indexer_for\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m   6163\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6164\u001b[0m \u001b[38;5;124;03mGuaranteed return of an indexer even when non-unique.\u001b[39;00m\n\u001b[1;32m   6165\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6178\u001b[0m \u001b[38;5;124;03marray([0, 2])\u001b[39;00m\n\u001b[1;32m   6179\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 6181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6182\u001b[0m indexer, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(target)\n\u001b[1;32m   6183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indexer\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Projects/clarkston_consulting/env/lib/python3.9/site-packages/pandas/core/indexes/base.py:3877\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3875\u001b[0m method \u001b[38;5;241m=\u001b[39m clean_reindex_fill_method(method)\n\u001b[1;32m   3876\u001b[0m orig_target \u001b[38;5;241m=\u001b[39m target\n\u001b[0;32m-> 3877\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_cast_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3879\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_method(method, limit, tolerance)\n\u001b[1;32m   3881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Projects/clarkston_consulting/env/lib/python3.9/site-packages/pandas/core/indexes/base.py:6682\u001b[0m, in \u001b[0;36mIndex._maybe_cast_listlike_indexer\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m   6678\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_cast_listlike_indexer\u001b[39m(\u001b[38;5;28mself\u001b[39m, target) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[1;32m   6679\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6680\u001b[0m \u001b[38;5;124;03m    Analogue to maybe_cast_indexer for get_indexer instead of get_loc.\u001b[39;00m\n\u001b[1;32m   6681\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 6682\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mensure_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Projects/clarkston_consulting/env/lib/python3.9/site-packages/pandas/core/indexes/base.py:7648\u001b[0m, in \u001b[0;36mensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   7646\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Index(index_like, copy\u001b[38;5;241m=\u001b[39mcopy, tupleize_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   7647\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 7648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Projects/clarkston_consulting/env/lib/python3.9/site-packages/pandas/core/indexes/base.py:565\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[1;32m    562\u001b[0m         data \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(data, dtype\u001b[38;5;241m=\u001b[39m_dtype_obj)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex must be specified when data is not list-like\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Projects/clarkston_consulting/env/lib/python3.9/site-packages/pandas/core/construction.py:606\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    604\u001b[0m subarr \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m--> 606\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_infer_to_datetimelike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    608\u001b[0m         object_index\n\u001b[1;32m    609\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m using_pyarrow_string_dtype()\n\u001b[1;32m    610\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m is_string_dtype(subarr)\n\u001b[1;32m    611\u001b[0m     ):\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;66;03m# Avoid inference when string option is set\u001b[39;00m\n\u001b[1;32m    613\u001b[0m         subarr \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Projects/clarkston_consulting/env/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1182\u001b[0m, in \u001b[0;36mmaybe_infer_to_datetimelike\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m(value))  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;66;03m# Caller is responsible\u001b[39;00m\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(value\u001b[38;5;241m.\u001b[39mndim)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value):\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[0;31mValueError\u001b[0m: 3"
     ]
    }
   ],
   "source": [
    "# Find columns with data integrity / formatting needs\n",
    "string_columns = [customer_extract_df.select_dtypes(include='object').columns]\n",
    "\n",
    "customer_extract_df[[string_columns]].replace(r'[^A-Z|a-z|0-9]', '', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file\n",
    "used_fields = customer_spec_df['extract_field_name'].loc[customer_spec_df['field_used'] == True]\n",
    "\n",
    "customer_extract_df[used_fields].to_csv('CustomerLoad.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Process\n",
    "\n",
    "<ul>\n",
    "<li>Convert the spec into a usable format</li>\n",
    "<li>Make the extract fields make sense</li>\n",
    "<li>Identify anything in the extract that will break the new system according to the spec (missing fields, bad types, truncation, etc)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Found Problems\n",
    "\n",
    "<ul>\n",
    "    <li>Account Group and Customer Account have ambiguous field names in the extract. There are two fields in the spec with distinct table/column name combos but the extract doesn't specify which is which.</li>\n",
    "    <li>Duplicate SAP Field Descriptions</li>\n",
    "    <li>Leading and trailing special characters for the field descriptions. This is only an issue due to converting the field names in the extract to friendly terms</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
